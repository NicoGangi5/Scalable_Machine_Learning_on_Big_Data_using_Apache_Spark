{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "W4_Assignment.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicoGangi5/Scalable_Machine_Learning_on_Big_Data_using_Apache_Spark/blob/main/W4_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8dHCQ2JFSw7",
        "outputId": "3c38563d-ed22-4ab4-9b46-df81ed330883"
      },
      "source": [
        "from IPython.display import Markdown, display\n",
        "def printmd(string):\n",
        "    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n",
        "\n",
        "\n",
        "if ('sc' in locals() or 'sc' in globals()):\n",
        "    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n",
        "\n",
        "    \n",
        "    \n",
        "!pip install pyspark==2.4.5\n",
        "\n",
        "\n",
        "try:\n",
        "    from pyspark import SparkContext, SparkConf\n",
        "    from pyspark.sql import SparkSession\n",
        "except ImportError as e:\n",
        "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')\n",
        "    \n",
        "    \n",
        "\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark==2.4.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (2.4.5)\n",
            "Requirement already satisfied: py4j==0.10.7 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pyspark==2.4.5) (0.10.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37RRFTv8FSxD",
        "outputId": "5415118a-51b3-4a72-badb-d9bcefc2be14"
      },
      "source": [
        "# delete files from previous runs\n",
        "!rm -rf noaa-weather-data-jfk-airport*\n",
        "\n",
        "# download the file containing the data in CSV format\n",
        "!wget https://dax-cdn.cdn.appdomain.cloud/dax-noaa-weather-data-jfk-airport/1.1.4/noaa-weather-data-jfk-airport.tar.gz\n",
        "\n",
        "# extract the data\n",
        "!tar xvfz noaa-weather-data-jfk-airport.tar.gz noaa-weather-data-jfk-airport/jfk_weather.csv\n",
        "    \n",
        "# create a dataframe out of it by using the first row as field names and trying to infer a schema based on contents\n",
        "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").csv('noaa-weather-data-jfk-airport/jfk_weather.csv')\n",
        "\n",
        "# register a corresponding query table\n",
        "df.createOrReplaceTempView('df')\n",
        "\n",
        "df.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-31 00:29:12--  https://dax-cdn.cdn.appdomain.cloud/dax-noaa-weather-data-jfk-airport/1.1.4/noaa-weather-data-jfk-airport.tar.gz\n",
            "Resolving dax-cdn.cdn.appdomain.cloud (dax-cdn.cdn.appdomain.cloud)... 88.221.135.49, 95.101.143.218, 2a02:26f0:a1::58dd:8731, ...\n",
            "Connecting to dax-cdn.cdn.appdomain.cloud (dax-cdn.cdn.appdomain.cloud)|88.221.135.49|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3509993 (3.3M) [application/x-tar]\n",
            "Saving to: ‘noaa-weather-data-jfk-airport.tar.gz’\n",
            "\n",
            "noaa-weather-data-j 100%[===================>]   3.35M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-10-31 00:29:12 (147 MB/s) - ‘noaa-weather-data-jfk-airport.tar.gz’ saved [3509993/3509993]\n",
            "\n",
            "noaa-weather-data-jfk-airport/jfk_weather.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "114545"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mnnW2GMFSxH"
      },
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "from pyspark.sql.functions import translate, col\n",
        "\n",
        "df_cleaned = df \\\n",
        "    .withColumn(\"HOURLYWindSpeed\", df.HOURLYWindSpeed.cast('double')) \\\n",
        "    .withColumn(\"HOURLYWindDirection\", df.HOURLYWindDirection.cast('double')) \\\n",
        "    .withColumn(\"HOURLYStationPressure\", translate(col(\"HOURLYStationPressure\"), \"s,\", \"\")) \\\n",
        "    .withColumn(\"HOURLYPrecip\", translate(col(\"HOURLYPrecip\"), \"s,\", \"\")) \\\n",
        "    .withColumn(\"HOURLYRelativeHumidity\", translate(col(\"HOURLYRelativeHumidity\"), \"*\", \"\")) \\\n",
        "    .withColumn(\"HOURLYDRYBULBTEMPC\", translate(col(\"HOURLYDRYBULBTEMPC\"), \"*\", \"\")) \\\n",
        "\n",
        "df_cleaned =   df_cleaned \\\n",
        "                    .withColumn(\"HOURLYStationPressure\", df_cleaned.HOURLYStationPressure.cast('double')) \\\n",
        "                    .withColumn(\"HOURLYPrecip\", df_cleaned.HOURLYPrecip.cast('double')) \\\n",
        "                    .withColumn(\"HOURLYRelativeHumidity\", df_cleaned.HOURLYRelativeHumidity.cast('double')) \\\n",
        "                    .withColumn(\"HOURLYDRYBULBTEMPC\", df_cleaned.HOURLYDRYBULBTEMPC.cast('double')) \\\n",
        "\n",
        "df_filtered = df_cleaned.filter(\"\"\"\n",
        "    HOURLYWindSpeed <> 0\n",
        "    and HOURLYWindSpeed IS NOT NULL\n",
        "    and HOURLYWindDirection IS NOT NULL\n",
        "    and HOURLYStationPressure IS NOT NULL\n",
        "    and HOURLYPressureTendency IS NOT NULL\n",
        "    and HOURLYPrecip IS NOT NULL\n",
        "    and HOURLYRelativeHumidity IS NOT NULL\n",
        "    and HOURLYDRYBULBTEMPC IS NOT NULL\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh0hypQ8FSxK",
        "outputId": "ca6fab2c-768e-49ad-c1b2-1c6e83688be5"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYWindDirection\",\"HOURLYStationPressure\"],\n",
        "                                  outputCol=\"features\")\n",
        "df_pipeline = vectorAssembler.transform(df_filtered)\n",
        "from pyspark.ml.stat import Correlation\n",
        "Correlation.corr(df_pipeline,\"features\").head()[0].toArray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.        ,  0.25478863, -0.26171147],\n",
              "       [ 0.25478863,  1.        , -0.13377466],\n",
              "       [-0.26171147, -0.13377466,  1.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjGbozD9FSxN"
      },
      "source": [
        "splits = df_filtered.randomSplit([0.8, 0.2])\n",
        "df_train = splits[0]\n",
        "df_test = splits[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjg_VjtjFSxP"
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import Normalizer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "vectorAssembler = VectorAssembler(inputCols=[\n",
        "                                    \"HOURLYWindDirection\",\n",
        "                                    \"ELEVATION\",\n",
        "                                    \"HOURLYStationPressure\"],\n",
        "                                  outputCol=\"features\")\n",
        "\n",
        "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjuBZrShFSxS"
      },
      "source": [
        "def regression_metrics(prediction):\n",
        "    from pyspark.ml.evaluation import RegressionEvaluator\n",
        "    evaluator = RegressionEvaluator(\n",
        "    labelCol=\"HOURLYWindSpeed\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "    rmse = evaluator.evaluate(prediction)\n",
        "    print(\"RMSE on test data = %g\" % rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW5zkWdvFSxV",
        "outputId": "39639b7e-db32-4872-e66c-d3646b8e9e39"
      },
      "source": [
        "#LR1\n",
        "\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "\n",
        "lr = LinearRegression(labelCol=\"HOURLYWindSpeed\", featuresCol='features_norm', maxIter=100, regParam=0.0, elasticNetParam=0.0)\n",
        "pipeline = Pipeline(stages=[vectorAssembler, normalizer,lr])\n",
        "model = pipeline.fit(df_train)\n",
        "prediction = model.transform(df_test)\n",
        "regression_metrics(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE on test data = 0.918626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3LqfKhVFSxX",
        "outputId": "55f3f65a-5d70-4887-9098-398019fc1889"
      },
      "source": [
        "#GBT1\n",
        "\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "\n",
        "gbt = GBTRegressor(labelCol=\"HOURLYWindSpeed\", maxIter=100)\n",
        "pipeline = Pipeline(stages=[vectorAssembler, normalizer,gbt])\n",
        "model = pipeline.fit(df_train)\n",
        "prediction = model.transform(df_test)\n",
        "regression_metrics(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE on test data = 5.11654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og1xpXAaFSxa"
      },
      "source": [
        "from pyspark.ml.feature import Bucketizer, OneHotEncoder\n",
        "bucketizer = Bucketizer(splits=[ 0, 180, float('Inf') ],inputCol=\"HOURLYWindDirection\", outputCol=\"HOURLYWindDirectionBucketized\")\n",
        "encoder = OneHotEncoder(inputCol=\"HOURLYWindDirectionBucketized\", outputCol=\"HOURLYWindDirectionOHE\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d68tp1T-FSxc"
      },
      "source": [
        "def classification_metrics(prediction):\n",
        "    from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "    mcEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"HOURLYWindDirectionBucketized\")\n",
        "    accuracy = mcEval.evaluate(prediction)\n",
        "    print(\"Accuracy on test data = %g\" % accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkVBFn8dFSxe",
        "outputId": "df4cedac-a797-4a3b-b79e-42b19c29e3ca"
      },
      "source": [
        "#LGReg1\n",
        "\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(labelCol=\"HOURLYWindDirectionBucketized\", maxIter=10)\n",
        "#,\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"\n",
        "\n",
        "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\"],\n",
        "                                  outputCol=\"features\")\n",
        "\n",
        "pipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,lr])\n",
        "model = pipeline.fit(df_train)\n",
        "prediction = model.transform(df_test)\n",
        "classification_metrics(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test data = 0.692922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8552u1hHFSxh",
        "outputId": "74ebf4c9-57c7-4c41-c291-d23827fc7542"
      },
      "source": [
        "#RF1\n",
        "\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(labelCol=\"HOURLYWindDirectionBucketized\", numTrees=10)\n",
        "\n",
        "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\",\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"],\n",
        "                                  outputCol=\"features\")\n",
        "\n",
        "pipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,rf])\n",
        "model = pipeline.fit(df_train)\n",
        "prediction = model.transform(df_test)\n",
        "classification_metrics(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test data = 0.722374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkPDN_cLFSxj",
        "outputId": "af161459-38b1-4d97-c266-60e8e55891b5"
      },
      "source": [
        "#GBT2\n",
        "\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "\n",
        "gbt = GBTClassifier(labelCol=\"HOURLYWindDirectionBucketized\", maxIter=100)\n",
        "\n",
        "vectorAssembler = VectorAssembler(inputCols=[\"HOURLYWindSpeed\",\"HOURLYDRYBULBTEMPC\",\"ELEVATION\",\"HOURLYStationPressure\",\"HOURLYPressureTendency\",\"HOURLYPrecip\"],\n",
        "                                  outputCol=\"features\")\n",
        "\n",
        "pipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,gbt])\n",
        "model = pipeline.fit(df_train)\n",
        "prediction = model.transform(df_test)\n",
        "classification_metrics(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test data = 0.733333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzjeY4NmFSxl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}