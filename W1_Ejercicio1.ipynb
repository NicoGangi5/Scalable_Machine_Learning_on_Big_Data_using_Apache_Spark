{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Ejercicio1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicoGangi5/Scalable_Machine_Learning_on_Big_Data_using_Apache_Spark/blob/main/W1_Ejercicio1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nm23aTq-3yf"
      },
      "source": [
        "from IPython.display import Markdown, display\n",
        "def printmd(string):\n",
        "    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n",
        "\n",
        "\n",
        "if ('sc' in locals() or 'sc' in globals()):\n",
        "    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHUv813M-3yn",
        "outputId": "f4ad36d2-a608-4653-fded-bd75d46041d0"
      },
      "source": [
        "!pip install pyspark==2.4.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark==2.4.5\n",
            "  Downloading pyspark-2.4.5.tar.gz (217.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 217.8 MB 71.1 MB/s eta 0:00:01     | 76.0 MB 65.7 MB/s eta 0:00:03\n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "  Downloading py4j-0.10.7-py2.py3-none-any.whl (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 63.5 MB/s eta 0:00:01\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257927 sha256=de9f4834ff68a2ec9dda31c29789816400ea42ed53b7b170b4f3ff650d8eb584\n",
            "  Stored in directory: /tmp/wsuser/.cache/pip/wheels/01/c0/03/1c241c9c482b647d4d99412a98a5c7f87472728ad41ae55e1e\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAaWV6Lb-3ys"
      },
      "source": [
        "try:\n",
        "    from pyspark import SparkContext, SparkConf\n",
        "    from pyspark.sql import SparkSession\n",
        "except ImportError as e:\n",
        "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGUJtz46-3yv"
      },
      "source": [
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-_HHk93-3yy"
      },
      "source": [
        "rdd = sc.parallelize(range(100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsdRCTKH-3y0",
        "outputId": "f05bd2a2-7351-4f9c-de59-ef29abfcc1e8"
      },
      "source": [
        "# please replace $$ with the correct characters\n",
        "rdd.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRVizKsA-3y3",
        "outputId": "321af6ed-0881-480b-baa1-9742e3b2b65d"
      },
      "source": [
        "rdd.sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4950"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}