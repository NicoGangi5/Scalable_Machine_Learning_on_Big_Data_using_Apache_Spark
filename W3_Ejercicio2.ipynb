{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "W3_Ejercicio2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicoGangi5/Scalable_Machine_Learning_on_Big_Data_using_Apache_Spark/blob/main/W3_Ejercicio2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iRWblhqDtTE",
        "outputId": "30b4dc5f-0aac-42a8-9891-cbc4f57b9bb4"
      },
      "source": [
        "from IPython.display import Markdown, display\n",
        "def printmd(string):\n",
        "    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n",
        "\n",
        "\n",
        "if ('sc' in locals() or 'sc' in globals()):\n",
        "    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n",
        "\n",
        "    \n",
        "    \n",
        "!pip install pyspark==2.4.5\n",
        "\n",
        "\n",
        "  \n",
        "try:\n",
        "    from pyspark import SparkContext, SparkConf\n",
        "    from pyspark.sql import SparkSession\n",
        "except ImportError as e:\n",
        "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')\n",
        "    \n",
        "    \n",
        "    \n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# <span style=\"color:red\"><<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>></span>"
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark==2.4.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (2.4.5)\r\n",
            "Requirement already satisfied: py4j==0.10.7 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pyspark==2.4.5) (0.10.7)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgKGVpZoDtTL",
        "outputId": "486c6225-8364-4c7f-9671-e2984638c142"
      },
      "source": [
        "# delete files from previous runs\n",
        "!rm -f hmp.parquet*\n",
        "\n",
        "# download the file containing the data in PARQUET format\n",
        "!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
        "    \n",
        "# create a dataframe out of it\n",
        "df = spark.read.parquet('hmp.parquet')\n",
        "\n",
        "# register a corresponding query table\n",
        "df.createOrReplaceTempView('df')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-30 17:13:05--  https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet [following]\n",
            "--2020-10-30 17:13:06--  https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet [following]\n",
            "--2020-10-30 17:13:06--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.16.133\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.16.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 932997 (911K) [application/octet-stream]\n",
            "Saving to: ‘hmp.parquet’\n",
            "\n",
            "hmp.parquet         100%[===================>] 911.13K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-10-30 17:13:06 (17.5 MB/s) - ‘hmp.parquet’ saved [932997/932997]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq2dOICEDtTP",
        "outputId": "d1eea187-d9f9-4297-a497-0b2273f5b7e8"
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Normalizer\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\n",
        "encoder = OneHotEncoder(inputCol=\"classIndex\", outputCol=\"categoryVec\")\n",
        "vectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n",
        "                                  outputCol=\"features\")\n",
        "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n",
        "\n",
        "pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer])\n",
        "model = pipeline.fit(df)\n",
        "prediction = model.transform(df)\n",
        "prediction.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n",
            "|  x|  y|  z|              source|      class|classIndex|   categoryVec|        features|       features_norm|\n",
            "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n",
            "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n",
            "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n",
            "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n",
            "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n",
            "| 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,52.0,34.0]|[0.19626168224299...|\n",
            "| 22| 51| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,34.0]|[0.20560747663551...|\n",
            "| 20| 50| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,50.0,35.0]|[0.19047619047619...|\n",
            "| 22| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,34.0]|[0.20370370370370...|\n",
            "| 22| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,50.0,34.0]|[0.20754716981132...|\n",
            "| 22| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,51.0,35.0]|[0.20370370370370...|\n",
            "| 21| 51| 33|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,51.0,33.0]|[0.2,0.4857142857...|\n",
            "| 20| 50| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,50.0,34.0]|[0.19230769230769...|\n",
            "| 21| 49| 33|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n",
            "| 21| 49| 33|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,49.0,33.0]|[0.20388349514563...|\n",
            "| 20| 51| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[20.0,51.0,35.0]|[0.18867924528301...|\n",
            "| 18| 49| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[18.0,49.0,34.0]|[0.17821782178217...|\n",
            "| 19| 48| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[19.0,48.0,34.0]|[0.18811881188118...|\n",
            "| 16| 53| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[16.0,53.0,34.0]|[0.15533980582524...|\n",
            "| 18| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[18.0,52.0,35.0]|[0.17142857142857...|\n",
            "| 18| 51| 32|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[18.0,51.0,32.0]|[0.17821782178217...|\n",
            "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quXf_a0RDtTS",
        "outputId": "5400f62e-dce1-40a7-d8b2-f1ac1e014b0a"
      },
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "\n",
        "Ks = 30\n",
        "for n in range(2,Ks):\n",
        "    kmeans = KMeans(featuresCol=\"features\").setK(n).setSeed(1)\n",
        "    pipeline = Pipeline(stages=[vectorAssembler, kmeans])\n",
        "    model = pipeline.fit(df)\n",
        "    predictions = model.transform(df)\n",
        "\n",
        "    evaluator = ClusteringEvaluator()\n",
        "\n",
        "    silhouette = evaluator.evaluate(predictions)\n",
        "    print(\"Silhouette with squared euclidean distance = \" + str(silhouette), \" with K = \", n)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Silhouette with squared euclidean distance = 0.6875664014387497  with K =  2\n",
            "Silhouette with squared euclidean distance = 0.6147915951361759  with K =  3\n",
            "Silhouette with squared euclidean distance = 0.6333227654128869  with K =  4\n",
            "Silhouette with squared euclidean distance = 0.5937447997439024  with K =  5\n",
            "Silhouette with squared euclidean distance = 0.592463658820136  with K =  6\n",
            "Silhouette with squared euclidean distance = 0.5484627422401509  with K =  7\n",
            "Silhouette with squared euclidean distance = 0.46686489256383346  with K =  8\n",
            "Silhouette with squared euclidean distance = 0.48034893889849645  with K =  9\n",
            "Silhouette with squared euclidean distance = 0.47370428136987536  with K =  10\n",
            "Silhouette with squared euclidean distance = 0.4819049717562352  with K =  11\n",
            "Silhouette with squared euclidean distance = 0.40964155503229643  with K =  12\n",
            "Silhouette with squared euclidean distance = 0.4153293521373778  with K =  13\n",
            "Silhouette with squared euclidean distance = 0.41244594513295846  with K =  14\n",
            "Silhouette with squared euclidean distance = 0.41771495579360896  with K =  15\n",
            "Silhouette with squared euclidean distance = 0.39594610810727193  with K =  16\n",
            "Silhouette with squared euclidean distance = 0.40512075095291467  with K =  17\n",
            "Silhouette with squared euclidean distance = 0.4058090075137995  with K =  18\n",
            "Silhouette with squared euclidean distance = 0.3794290531790819  with K =  19\n",
            "Silhouette with squared euclidean distance = 0.3445366343500456  with K =  20\n",
            "Silhouette with squared euclidean distance = 0.37382141208025593  with K =  21\n",
            "Silhouette with squared euclidean distance = 0.3755327082866356  with K =  22\n",
            "Silhouette with squared euclidean distance = 0.39582196537765696  with K =  23\n",
            "Silhouette with squared euclidean distance = 0.37043032167020984  with K =  24\n",
            "Silhouette with squared euclidean distance = 0.3648804382298117  with K =  25\n",
            "Silhouette with squared euclidean distance = 0.3676138362694509  with K =  26\n",
            "Silhouette with squared euclidean distance = 0.3774439790161244  with K =  27\n",
            "Silhouette with squared euclidean distance = 0.3716147628721412  with K =  28\n",
            "Silhouette with squared euclidean distance = 0.37914606071424267  with K =  29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEGWBgOZDtTV",
        "outputId": "1a7d5722-9cf0-4f70-f6f9-b0a02d0583a0"
      },
      "source": [
        "kmeans = KMeans(featuresCol=\"features\").setK(n).setSeed(1)\n",
        "pipeline = Pipeline(stages=[vectorAssembler, kmeans])\n",
        "model = pipeline.fit(df)\n",
        "predictions = model.transform(df)\n",
        "\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Silhouette with squared euclidean distance = 0.592463658820136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GBw0p5-DtTY"
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "df_denormalized = df.select([col('*'),(col('x')*10)]).drop('x').withColumnRenamed('(x * 10)','x')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvaNJYE5DtTb",
        "outputId": "d166124d-6d37-4cdd-a010-5e5b83967f96"
      },
      "source": [
        "kmeans = KMeans(featuresCol=\"features\").setK(14).setSeed(1)\n",
        "pipeline = Pipeline(stages=[vectorAssembler, kmeans])\n",
        "model = pipeline.fit(df_denormalized)\n",
        "predictions = model.transform(df_denormalized)\n",
        "\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Silhouette with squared euclidean distance = 0.5709023393004293\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}