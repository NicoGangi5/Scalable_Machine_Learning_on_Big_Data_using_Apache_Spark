{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "W2_Ejercicio1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicoGangi5/Scalable_Machine_Learning_on_Big_Data_using_Apache_Spark/blob/main/W2_Ejercicio1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KRLs6bvByXn"
      },
      "source": [
        "from IPython.display import Markdown, display\n",
        "def printmd(string):\n",
        "    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n",
        "\n",
        "\n",
        "if ('sc' in locals() or 'sc' in globals()):\n",
        "    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA9GUj8EByXt",
        "outputId": "0870066d-da6e-457c-9b88-8d82f4b9b3db"
      },
      "source": [
        "!pip install pyspark==2.4.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark==2.4.5\n",
            "  Downloading pyspark-2.4.5.tar.gz (217.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 217.8 MB 11 kB/s s eta 0:00:01\n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "  Downloading py4j-0.10.7-py2.py3-none-any.whl (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 37.6 MB/s eta 0:00:01\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257927 sha256=1f7852869a3f3d7332ff8ead79249b4d104a9e1cd09ca3690dd52db836da8476\n",
            "  Stored in directory: /tmp/wsuser/.cache/pip/wheels/01/c0/03/1c241c9c482b647d4d99412a98a5c7f87472728ad41ae55e1e\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTj3M2frByXy"
      },
      "source": [
        "try:\n",
        "    from pyspark import SparkContext, SparkConf\n",
        "    from pyspark.sql import SparkSession\n",
        "except ImportError as e:\n",
        "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvPTaJa5ByX1"
      },
      "source": [
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "5lLKzlM2ByX4",
        "outputId": "1cdaf144-3aab-4b9e-fb95-72ca06092c10"
      },
      "source": [
        "# delete files from previous runs\n",
        "!rm -f hmp.parquet*\n",
        "\n",
        "# download the file containing the data in PARQUET format\n",
        "!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
        "    \n",
        "# create a dataframe out of it\n",
        "df = spark.read.parquet('hmp.parquet')\n",
        "\n",
        "# register a corresponding query table\n",
        "df.createOrReplaceTempView('df')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-29 12:55:49--  https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet [following]\n",
            "--2020-10-29 12:55:49--  https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet [following]\n",
            "--2020-10-29 12:55:49--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.16.133\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.16.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 932997 (911K) [application/octet-stream]\n",
            "Saving to: ‘hmp.parquet’\n",
            "\n",
            "hmp.parquet         100%[===================>] 911.13K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-10-29 12:55:50 (20.6 MB/s) - ‘hmp.parquet’ saved [932997/932997]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-9TN0aUByX6",
        "outputId": "da26767b-cfb5-4e77-ca46-ec584f21c78f"
      },
      "source": [
        "df.show()\n",
        "df.printSchema()\n",
        "df.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+--------------------+-----------+\n",
            "|  x|  y|  z|              source|      class|\n",
            "+---+---+---+--------------------+-----------+\n",
            "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 51| 34|Accelerometer-201...|Brush_teeth|\n",
            "| 20| 50| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 52| 34|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 50| 34|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 51| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 21| 51| 33|Accelerometer-201...|Brush_teeth|\n",
            "| 20| 50| 34|Accelerometer-201...|Brush_teeth|\n",
            "| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n",
            "| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n",
            "| 20| 51| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 18| 49| 34|Accelerometer-201...|Brush_teeth|\n",
            "| 19| 48| 34|Accelerometer-201...|Brush_teeth|\n",
            "| 16| 53| 34|Accelerometer-201...|Brush_teeth|\n",
            "| 18| 52| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 18| 51| 32|Accelerometer-201...|Brush_teeth|\n",
            "+---+---+---+--------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- x: integer (nullable = true)\n",
            " |-- y: integer (nullable = true)\n",
            " |-- z: integer (nullable = true)\n",
            " |-- source: string (nullable = true)\n",
            " |-- class: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "446529"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_WIYMRJByX-",
        "outputId": "fded8e2d-e925-46f9-9ccb-5cc102ec676e"
      },
      "source": [
        "spark.sql('select class,count(*) from df group by class').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------+--------+\n",
            "|         class|count(1)|\n",
            "+--------------+--------+\n",
            "| Use_telephone|   15225|\n",
            "| Standup_chair|   25417|\n",
            "|      Eat_meat|   31236|\n",
            "|     Getup_bed|   45801|\n",
            "|   Drink_glass|   42792|\n",
            "|    Pour_water|   41673|\n",
            "|     Comb_hair|   23504|\n",
            "|          Walk|   92254|\n",
            "|  Climb_stairs|   40258|\n",
            "| Sitdown_chair|   25036|\n",
            "|   Liedown_bed|   11446|\n",
            "|Descend_stairs|   15375|\n",
            "|   Brush_teeth|   29829|\n",
            "|      Eat_soup|    6683|\n",
            "+--------------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXnUIF3kByYC",
        "outputId": "ab72da1d-9ed7-4de2-f493-885d399422e0"
      },
      "source": [
        "df.groupBy('class').count().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------+-----+\n",
            "|         class|count|\n",
            "+--------------+-----+\n",
            "| Use_telephone|15225|\n",
            "| Standup_chair|25417|\n",
            "|      Eat_meat|31236|\n",
            "|     Getup_bed|45801|\n",
            "|   Drink_glass|42792|\n",
            "|    Pour_water|41673|\n",
            "|     Comb_hair|23504|\n",
            "|          Walk|92254|\n",
            "|  Climb_stairs|40258|\n",
            "| Sitdown_chair|25036|\n",
            "|   Liedown_bed|11446|\n",
            "|Descend_stairs|15375|\n",
            "|   Brush_teeth|29829|\n",
            "|      Eat_soup| 6683|\n",
            "+--------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "dQmydlffByYE",
        "outputId": "0500368b-73c5-4b48-9f9c-cef15dfd0380"
      },
      "source": [
        "!pip install pixiedust"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pixiedust\n",
            "  Downloading pixiedust-1.1.18.tar.gz (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 8.9 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting mpld3\n",
            "  Downloading mpld3-0.5.1.tar.gz (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 22.7 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: lxml in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pixiedust) (4.5.1)\n",
            "Collecting geojson\n",
            "  Downloading geojson-2.5.0-py2.py3-none-any.whl (14 kB)\n",
            "Collecting astunparse\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: markdown in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pixiedust) (3.1.1)\n",
            "Collecting colour\n",
            "  Downloading colour-0.1.5-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pixiedust) (2.24.0)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from mpld3->pixiedust) (2.11.2)\n",
            "Requirement already satisfied: matplotlib in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from mpld3->pixiedust) (3.2.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from astunparse->pixiedust) (0.34.2)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from astunparse->pixiedust) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=36 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from markdown->pixiedust) (47.3.1.post20200622)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests->pixiedust) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests->pixiedust) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests->pixiedust) (1.25.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests->pixiedust) (2.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from jinja2->mpld3->pixiedust) (1.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib->mpld3->pixiedust) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib->mpld3->pixiedust) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib->mpld3->pixiedust) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib->mpld3->pixiedust) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib->mpld3->pixiedust) (1.18.5)\n",
            "Building wheels for collected packages: pixiedust, mpld3\n",
            "  Building wheel for pixiedust (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pixiedust: filename=pixiedust-1.1.18-py3-none-any.whl size=321728 sha256=ddafe6d95eacd788932f28c0a64067a4839e9903e5bcb105b04d26dd664e366d\n",
            "  Stored in directory: /tmp/wsuser/.cache/pip/wheels/41/4c/20/08a843440aaeffc976c1848c9eb44be6ec68dcd964421ec6f7\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for mpld3: filename=mpld3-0.5.1-py3-none-any.whl size=364062 sha256=9ccc8e1b7402a607d8750c7652f5f01a5371c9f63e7278d9e884493ce4a704d4\n",
            "  Stored in directory: /tmp/wsuser/.cache/pip/wheels/b9/36/27/d61b9f1327012961fa31f05a20b190f836dd3fcb1c0264177b\n",
            "Successfully built pixiedust mpld3\n",
            "Installing collected packages: mpld3, geojson, astunparse, colour, pixiedust\n",
            "Successfully installed astunparse-1.6.3 colour-0.1.5 geojson-2.5.0 mpld3-0.5.1 pixiedust-1.1.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "JW2cCF0YByYH",
        "outputId": "c8fbc646-8a74-4c40-c952-40608b171c6b"
      },
      "source": [
        "import pixiedust\n",
        "from pyspark.sql.functions import col\n",
        "counts = df.groupBy('class').count().orderBy('count')\n",
        "display(counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pixiedust database opened successfully\n",
            "Table VERSION_TRACKER created successfully\n",
            "Table METRICS_TRACKER created successfully\n",
            "\n",
            "Share anonymous install statistics? (opt-out instructions)\n",
            "\n",
            "PixieDust will record metadata on its environment the next time the package is installed or updated. The data is anonymized and aggregated to help plan for future releases, and records only the following values:\n",
            "\n",
            "{\n",
            "   \"data_sent\": currentDate,\n",
            "   \"runtime\": \"python\",\n",
            "   \"application_version\": currentPixiedustVersion,\n",
            "   \"space_id\": nonIdentifyingUniqueId,\n",
            "   \"config\": {\n",
            "       \"repository_id\": \"https://github.com/ibm-watson-data-lab/pixiedust\",\n",
            "       \"target_runtimes\": [\"Data Science Experience\"],\n",
            "       \"event_id\": \"web\",\n",
            "       \"event_organizer\": \"dev-journeys\"\n",
            "   }\n",
            "}\n",
            "You can opt out by calling pixiedust.optOut() in a new cell.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <div style=\"margin:10px\">\n",
              "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
              "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
              "            </a>\n",
              "            <span>Pixiedust version 1.1.18</span>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mPixiedust runtime updated. Please restart kernel\u001b[0m\n",
            "Table SPARK_PACKAGES created successfully\n",
            "Table USER_PREFERENCES created successfully\n",
            "Table service_connections created successfully\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[class: string, count: bigint]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4VL4IPZByYJ",
        "outputId": "6bcddf00-3cf7-45d7-98b4-49b49777b073"
      },
      "source": [
        "spark.sql('''\n",
        "    select \n",
        "        *,\n",
        "        max/min as minmaxratio -- compute minmaxratio based on previously computed values\n",
        "        from (\n",
        "            select \n",
        "                min(ct) as min, -- compute minimum value of all classes\n",
        "                max(ct) as max, -- compute maximum value of all classes\n",
        "                mean(ct) as mean, -- compute mean between all classes\n",
        "                stddev(ct) as stddev -- compute standard deviation between all classes\n",
        "                from (\n",
        "                    select\n",
        "                        count(*) as ct -- count the number of rows per class and rename it to ct\n",
        "                        from df -- access the temporary query table called df backed by DataFrame df\n",
        "                        group by class -- aggrecate over class\n",
        "                )\n",
        "        )   \n",
        "''').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+------------------+------------------+-----------------+\n",
            "| min|  max|              mean|            stddev|      minmaxratio|\n",
            "+----+-----+------------------+------------------+-----------------+\n",
            "|6683|92254|31894.928571428572|21284.893716741157|13.80427951518779|\n",
            "+----+-----+------------------+------------------+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k220QfD1ByYL",
        "outputId": "ec51cb22-b115-4926-cfac-d9a3d56d610a"
      },
      "source": [
        "df.groupBy('class').count().sort('count').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------+-----+\n",
            "|         class|count|\n",
            "+--------------+-----+\n",
            "|      Eat_soup| 6683|\n",
            "|   Liedown_bed|11446|\n",
            "| Use_telephone|15225|\n",
            "|Descend_stairs|15375|\n",
            "|     Comb_hair|23504|\n",
            "| Sitdown_chair|25036|\n",
            "| Standup_chair|25417|\n",
            "|   Brush_teeth|29829|\n",
            "|      Eat_meat|31236|\n",
            "|  Climb_stairs|40258|\n",
            "|    Pour_water|41673|\n",
            "|   Drink_glass|42792|\n",
            "|     Getup_bed|45801|\n",
            "|          Walk|92254|\n",
            "+--------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f86s7O2gByYN"
      },
      "source": [
        "from pyspark.sql.functions import min\n",
        "\n",
        "# create a lot of distinct classes from the dataset\n",
        "classes = [row[0] for row in df.select('class').distinct().collect()]\n",
        "\n",
        "# compute the number of elements of the smallest class in order to limit the number of samples per calss\n",
        "min = df.groupBy('class').count().select(min('count')).first()[0]\n",
        "\n",
        "# define the result dataframe variable\n",
        "df_balanced = None\n",
        "\n",
        "# iterate over distinct classes\n",
        "for cls in classes:\n",
        "    \n",
        "    # only select examples for the specific class within this iteration\n",
        "    # shuffle the order of the elements (by setting fraction to 1.0 sample works like shuffle)\n",
        "    # return only the first n samples\n",
        "    df_temp = df \\\n",
        "        .filter(\"class = '\"+cls+\"'\") \\\n",
        "        .sample(False, 1.0) \\\n",
        "        .limit(min)\n",
        "    \n",
        "    # on first iteration, assing df_temp to empty df_balanced\n",
        "    if df_balanced == None:    \n",
        "        df_balanced = df_temp\n",
        "    # afterwards, append vertically\n",
        "    else:\n",
        "        df_balanced=df_balanced.union(df_temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8sEqMQHByYP",
        "outputId": "9e282012-1243-4e08-becd-ea1043548e52"
      },
      "source": [
        "df_balanced.groupBy('class').count().sort('count').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------+-----+\n",
            "|         class|count|\n",
            "+--------------+-----+\n",
            "| Use_telephone| 6683|\n",
            "| Standup_chair| 6683|\n",
            "|      Eat_meat| 6683|\n",
            "|     Getup_bed| 6683|\n",
            "|   Drink_glass| 6683|\n",
            "|    Pour_water| 6683|\n",
            "|     Comb_hair| 6683|\n",
            "|          Walk| 6683|\n",
            "|  Climb_stairs| 6683|\n",
            "|   Liedown_bed| 6683|\n",
            "| Sitdown_chair| 6683|\n",
            "|Descend_stairs| 6683|\n",
            "|   Brush_teeth| 6683|\n",
            "|      Eat_soup| 6683|\n",
            "+--------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}